[
  {
    "objectID": "causal-targets/causal-targets.html#we-do-a-bad-job-teaching-statistics",
    "href": "causal-targets/causal-targets.html#we-do-a-bad-job-teaching-statistics",
    "title": "The Causal Roadmap",
    "section": "We do a bad job teaching statistics",
    "text": "We do a bad job teaching statistics"
  },
  {
    "objectID": "causal-targets/causal-targets.html#sound-familiar",
    "href": "causal-targets/causal-targets.html#sound-familiar",
    "title": "The Causal Roadmap",
    "section": "Sound familiar?",
    "text": "Sound familiar?\nWhat type of data do you have?\n\nContinuous \\(\\rightarrow\\) linear regression (but only if it’s normally distributed!)\nBinary \\(\\rightarrow\\) logistic regression\nTime to event \\(\\rightarrow\\) proportional hazards regression\n\nThis approach serves you poorly because it does not reflect how we expect you to do research"
  },
  {
    "objectID": "causal-targets/causal-targets.html#overview",
    "href": "causal-targets/causal-targets.html#overview",
    "title": "The Causal Roadmap",
    "section": "Overview",
    "text": "Overview\n\nTranslate the scientific question of interest into a formal causal parameter\n\noptional: define the ideal study\n\nSpecify a model for the generating mechanism of the observed data, with the causal parameter in mind\n\ni.e., a DAG and some other assumptions\n\nState the identifying assumptions, and, derive the statistical parameter\n\nunder what conditions can the observed data narrow down the causal parameter to a single value?\n\nSpecify statistical model, the thing you will use to estimate the statistical parameter and account for sampling variability"
  },
  {
    "objectID": "causal-targets/causal-targets.html#overview-2",
    "href": "causal-targets/causal-targets.html#overview-2",
    "title": "The Causal Roadmap",
    "section": "Overview (2)",
    "text": "Overview (2)\n\nEstimation and interpretation\nDerive causal bounds for the causal parameter when not identified\n\naddress sensitivity of results to the identifying assumptions\n\n\nPlans\nI’d like to briefly review 1-4, and focus mainly on the “statistical gap”, number 5. Hopefully to help “what model should I use?”\nIf you are interested, a lot of my own research is about 6."
  },
  {
    "objectID": "causal-targets/causal-targets.html#causal-inference",
    "href": "causal-targets/causal-targets.html#causal-inference",
    "title": "The Causal Roadmap",
    "section": "Causal inference",
    "text": "Causal inference\n\nCausal effects can be thought of as the change in the summary statistic of an outcome variable associated with the manipulation of an exposure variable from/to a particular level.\nWe have observations, which are usually generated via mechanisms that do not involve manipulation of the exposure.\nThe goal is to use the distribution of the observations to inform us about the causal effect of interest"
  },
  {
    "objectID": "causal-targets/causal-targets.html#dags-and-observations",
    "href": "causal-targets/causal-targets.html#dags-and-observations",
    "title": "The Causal Roadmap",
    "section": "DAGs and observations",
    "text": "DAGs and observations\nA directed acyclic graph (or DAG or just graph) conveys our assumptions about the mechanisms that gave rise to the observations, e.g.,\n\nThis is a functional causal model \\(\\{F_V:pa(V)\\times U_V\\to V\\mid V\\in\\mathcal{V}\\}\\), e.g., \\(y = F_Y(x, u, \\varepsilon_Y)\\).\n\n\n\n\n\n\nNote\n\n\nThere are other frameworks for specifying a causal model, this is just one of them (the “Pearl model”, aka, NPSEM-IE).\nThe main other one that has led to many results in the causal literature is the Neyman/Rubin potential outcomes framework."
  },
  {
    "objectID": "causal-targets/causal-targets.html#effects",
    "href": "causal-targets/causal-targets.html#effects",
    "title": "The Causal Roadmap",
    "section": "Effects",
    "text": "Effects\n\nA causal effect will be written as e.g., \\(p\\{Y(X = 1) = 1\\} - p\\{Y(X = 0) = 1\\}\\), where \\(Y(X = 1)\\) is the potential outcome which means\n\n\nthe variable \\(Y\\) if \\(X\\) were intervened upon to have value 1.\n\n\nThink of the DAG where \\(X\\) has no parents, i.e., \\(X\\) depends on nothing other than what we set it to\nThis is only a example of a causal contrast, a comparison of a feature of the outcome under different interventions"
  },
  {
    "objectID": "causal-targets/causal-targets.html#example",
    "href": "causal-targets/causal-targets.html#example",
    "title": "The Causal Roadmap",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nThe scientific question\n\n\nDoes drinking coffee in childhood stunt growth?\n\n\n\n\nDesign an ideal study to answer this question. Ignore all restrictions due to ethics, resources, time, etc.\n\nstudy population\nintervention\nhow you would measure the effect\ndraw the DAG for this hypothetical study"
  },
  {
    "objectID": "causal-targets/causal-targets.html#example-continued",
    "href": "causal-targets/causal-targets.html#example-continued",
    "title": "The Causal Roadmap",
    "section": "Example continued",
    "text": "Example continued\nWhat causal parameter did you choose?\n\n\n\n\n\n\nNote\n\n\ncausal target, causal contrast, causal metric, all mean the same thing\n\n\n\nSome options:\n\nAverage causal effect: \\(E\\{Y(1)\\} - E\\{Y(0)\\}\\)\nConditional average causal effect: \\(E\\{Y(1) | X = x\\} - E\\{Y(0) | X = x\\}\\)\nIf \\(Y\\) is binary, the risk difference: \\(P\\{Y(1) = 1\\} - P\\{Y(0) = 1\\}\\)\n\nRatio: \\(P\\{Y(1) = 1\\}/P\\{Y(0) = 1\\}\\)\nNumber needed to treat: \\(1 / [P\\{Y(1) = 1\\} - P\\{Y(0) = 1\\}]\\)\nOdds ratio:\n\n\n\\[\n\\frac{P\\{Y(1) = 1\\}/P\\{Y(1) = 0\\}}{P\\{Y(0) = 1\\}/P\\{Y(0) = 0\\}}\n\\]"
  },
  {
    "objectID": "causal-targets/causal-targets.html#what-makes-a-good-causal-parameter",
    "href": "causal-targets/causal-targets.html#what-makes-a-good-causal-parameter",
    "title": "The Causal Roadmap",
    "section": "What makes a good causal parameter?",
    "text": "What makes a good causal parameter?\nThe answer is complicated, see Colnet et al. (2023) Risk ratio, odds ratio, risk difference… Which causal measure is easier to generalize?\n\n\n\nhttps://arxiv.org/pdf/2303.16008.pdf\n\n\nSome key tips:\n\nChoose something meaningful/interpretable to you, consider the statistical distribution of the outcome and what would be useful summary statistics (e.g., mean, median, probability of exceeding a threshold)\nDo not use the odds nor hazard ratio because it they are not logic respecting, i.e., the measure in the full population can be outside the range of measures defined in subpopulations (generally considered to be a paradox)"
  },
  {
    "objectID": "causal-targets/causal-targets.html#example-continued-1",
    "href": "causal-targets/causal-targets.html#example-continued-1",
    "title": "The Causal Roadmap",
    "section": "Example continued",
    "text": "Example continued\nNow consider the real world data. Suppose we use the conscription register in Sweden to answer this question. The observed data include\n\nHeight measured at age 18 - 20 in people joining the military\nSelf-reported coffee consumption before age 12\nEverything else available in the registers (e.g., highest education level of parents, hospitalizations and prescribed drugs)\n\nDraw the DAG for the data generating mechanism"
  },
  {
    "objectID": "causal-targets/causal-targets.html#identifiability",
    "href": "causal-targets/causal-targets.html#identifiability",
    "title": "The Causal Roadmap",
    "section": "Identifiability",
    "text": "Identifiability\nIs the effect of interest identifiable from these data?\nRules of thumb:\n\nIf the two variables of interest share an unmeasured common cause, the effect is not generally identified (unmeasured confounding)\nIf the selection mechanism has both the exposure and outcome as parents, the effect is not generally identified (selection/collider bias)"
  },
  {
    "objectID": "causal-targets/causal-targets.html#identifiability-continued",
    "href": "causal-targets/causal-targets.html#identifiability-continued",
    "title": "The Causal Roadmap",
    "section": "Identifiability continued",
    "text": "Identifiability continued\nFear not, there are algorithms that, given a DAG and an effect, determine whether it is identified and if so, give the statistical estimand (Tian and Pearl 2002 -)\nIf the effect is identifiable, the estimand is some variation on the g-formula:\n\\[\nE\\{Y(a)\\} = \\sum_x E(Y | A = a, X = x) P(X = x | A = a).\n\\]\n\n\n\n\n\n\nImportant\n\n\nThis still involves unknown quantities: the mean outcome given covariates and observed treatment, and the distribution of covariates given treatment.\nWe need to estimate these things using statistical models. Which ones and how?\n\n\n\nThis is the statistical gap"
  },
  {
    "objectID": "causal-targets/causal-targets.html#closing-the-statistical-gap",
    "href": "causal-targets/causal-targets.html#closing-the-statistical-gap",
    "title": "The Causal Roadmap",
    "section": "Closing the statistical gap",
    "text": "Closing the statistical gap\n\nRegression models and adjustment\nPropensity scores (weighting, adjustment, matching, …)\nDoubly robust estimation\nTargeted minimum loss estimation\nDouble machine learning\n…"
  },
  {
    "objectID": "causal-targets/causal-targets.html#testing-your-understanding",
    "href": "causal-targets/causal-targets.html#testing-your-understanding",
    "title": "The Causal Roadmap",
    "section": "Testing your understanding",
    "text": "Testing your understanding\nTrue or false?\n\nMike believes that you should never use logistic regression.\nDAGs are the only way to convey your causal assumptions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentations",
    "section": "",
    "text": "Hazards of hazard ratios\n\n\nCausal inference series\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nRegression and Standardization\n\n\nCausal inference series\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nThe Causal Roadmap\n\n\nCausal inference series\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "regression/regression.html#example",
    "href": "regression/regression.html#example",
    "title": "Regression and Standardization",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nThe scientific question\n\n\nDoes drinking coffee in childhood stunt growth?\n\n\n\nIn the ideal study\n\nWe would enroll children aged 10 - 12\nRandomize them to drink coffee versus placebo\nFollow them up for 8 years (very closely to ensure compliance)\nMeasure height at age 18 - 20\nCausal parameter of interest is\n\n\\[\nE\\{Y(X = 1)\\} - E\\{Y(X = 0)\\}\n\\]"
  },
  {
    "objectID": "regression/regression.html#identifiability",
    "href": "regression/regression.html#identifiability",
    "title": "Regression and Standardization",
    "section": "Identifiability",
    "text": "Identifiability\nIs the effect of interest identifiable from these data?\nYes! The dag is simple in this case:"
  },
  {
    "objectID": "regression/regression.html#estimation",
    "href": "regression/regression.html#estimation",
    "title": "Regression and Standardization",
    "section": "Estimation",
    "text": "Estimation\nOur usual estimand is\n\\[\nE(Y | X = 1) - E(Y | X = 0).\n\\]\nThis defines an estimator of the causal parameter because\n\\[\nE(Y | X = x) = E\\{Y(X = x) | X = x\\}\n\\] which is always true (sometimes called the consistency assumption/axiom).\nBecause of randomization, the potential outcomes are independent of the treatment assignment: \\[\nE\\{Y(X = x) | X = x\\} = E\\{Y(X = x)\\}\n\\]"
  },
  {
    "objectID": "regression/regression.html#estimation-continued",
    "href": "regression/regression.html#estimation-continued",
    "title": "Regression and Standardization",
    "section": "Estimation continued",
    "text": "Estimation continued\nAn estimator is \\[\n\\frac{\\sum_i Y_i X_i}{\\sum_i X_i} - \\frac{\\sum_i Y_i (1 - X_i)}{\\sum_i (1 - X_i)}\n\\] which is what you get from a t-test.\nThis is not the only possible estimator. It is consistent for the causal effect of interest, which means that as the sample size increases, the estimator will approach the truth. There are other properties to consider\n\nrobustness, sensitivity of the estimator to the assumptions (not so relevant for this example)\nefficiency, how precise the estimator is compared to alternatives"
  },
  {
    "objectID": "regression/regression.html#alternative-estimand",
    "href": "regression/regression.html#alternative-estimand",
    "title": "Regression and Standardization",
    "section": "Alternative estimand",
    "text": "Alternative estimand\nLinear regression adjusted for \\(C\\):\n\\[\nE(Y | X, C) = \\beta_0 + \\beta_1 X + \\beta_2 C\n\\]\nAs long as there is no interaction, \\(\\beta_1\\) has the interpretation \\(E(Y | X = 1) - E(Y | X = 0)\\), so what’s the difference?\nThe estimator is different from the t-test, and it involves \\(C\\)"
  },
  {
    "objectID": "regression/regression.html#simulation",
    "href": "regression/regression.html#simulation",
    "title": "Regression and Standardization",
    "section": "Simulation",
    "text": "Simulation\n\ngenerate_ests <- function(n = 300) {\n  X <- rbinom(n, 1, .5)\n  C <- rnorm(n)\n  Y <- 1 + 1 * X + C + rnorm(n)\n  \n  c(ttest = t.test(Y ~ X)$estimate |> diff(), \n    lm = lm(Y ~ X + C)$coefficients[2])\n\n}\n\nsimul <- t(replicate(1e3, generate_ests()))\n\nsummary(simul) \n\n ttest.mean in group 1      lm.X       \n Min.   :0.4874        Min.   :0.6407  \n 1st Qu.:0.8882        1st Qu.:0.9159  \n Median :0.9962        Median :0.9953  \n Mean   :0.9969        Mean   :0.9968  \n 3rd Qu.:1.1065        3rd Qu.:1.0707  \n Max.   :1.4630        Max.   :1.3408  \n\napply(simul, 2, sd)\n\nttest.mean in group 1                  lm.X \n            0.1586819             0.1145141 \n\n\nIt gives the same answer, and is much more efficient. What are the drawbacks?\nWe need to get the outcome model correct, so it is potentially less robust.\nHowever, linear regression is quite robust, it is hard to get the outcome model wrong enough to make a difference."
  },
  {
    "objectID": "regression/regression.html#simulation-2",
    "href": "regression/regression.html#simulation-2",
    "title": "Regression and Standardization",
    "section": "Simulation 2",
    "text": "Simulation 2\n\ngenerate_ests2 <- function(n = 300) {\n  X <- rbinom(n, 1, .5)\n  C <- rnorm(n)\n  U <- rnorm(n)\n  Y <- 1 + 1 * X + 5 * X * (C > 1) - 3 * (C < 1) + rnorm(n)\n  \n  c(ttest = t.test(Y ~ X)$estimate |> diff(), \n    lm = lm(Y ~ X + C)$coefficients[2])\n\n}\n\nsimul <- t(replicate(1e3, generate_ests2()))\n\nsummary(simul) \n\n ttest.mean in group 1      lm.X      \n Min.   :0.979         Min.   :1.158  \n 1st Qu.:1.625         1st Qu.:1.639  \n Median :1.819         Median :1.804  \n Mean   :1.815         Mean   :1.794  \n 3rd Qu.:1.995         3rd Qu.:1.950  \n Max.   :2.655         Max.   :2.573  \n\napply(simul, 2, sd)\n\nttest.mean in group 1                  lm.X \n            0.2822757             0.2317679 \n\n\n\n\n\n\n\n\nWarning\n\n\nLessons from linear regression do not often apply to other types of regression. For example, the coefficient in an adjusted logistic model is not an estimand for the causal difference."
  },
  {
    "objectID": "regression/regression.html#a-confounded-setting",
    "href": "regression/regression.html#a-confounded-setting",
    "title": "Regression and Standardization",
    "section": "A confounded setting",
    "text": "A confounded setting\nThe more typical scenario since we can’t force children to drink coffee:\n\nHow do we estimate this now?"
  },
  {
    "objectID": "regression/regression.html#the-g-forumla",
    "href": "regression/regression.html#the-g-forumla",
    "title": "Regression and Standardization",
    "section": "The g-forumla",
    "text": "The g-forumla\nLet’s focus on the first term \\(E\\{Y(X = 1)\\}\\)\n\\[\nE\\{Y(1)\\} = \\sum_c E(Y(1) | C = c) P(C = c) = \\sum_c E(Y(1) | C = c, X = 1) P(C = c)\n\\] \\[\n= \\sum_c E(Y | C = c, X = 1) P(C = c)\n\\] under the assumptions in the dag.\nThe first component inside the sum is an outcome model, we can use a prediction from linear regression.\nFor the second component, we can use the empirical distribution. This is based on the random sampling principle: if I have \\(n\\) subjects in my sample, and the \\(i\\) th subject’s covariates are \\(C_i\\), my best guess of the probability of observing data like \\(C_i\\) is \\(1/n\\)"
  },
  {
    "objectID": "regression/regression.html#regression-standardization",
    "href": "regression/regression.html#regression-standardization",
    "title": "Regression and Standardization",
    "section": "Regression standardization",
    "text": "Regression standardization\nThe regression standardization estimator of the g-formula is\n\\[\n\\hat{E}\\{Y(1)\\} = \\sum_{i=1}^n \\hat{E}(Y | X = 1, C = C_i) \\cdot \\frac{1}{n}.\n\\]\nThe basic approach:\n\nFit an outcome model involving the exposure of interest and confounders\nCopy the data used to fit the model and then:\n\nReplace observed values of \\(X\\) with 1, and get predictions from the model: \\(\\hat{E}(Y_i(1))\\)\nReplace observed values of \\(X\\) with 0, and get predictions from the model: \\(\\hat{E}(Y_i(0))\\)\n\nTake the mean and then the difference: \\(\\frac{1}{n}\\sum_i \\hat{E}(Y_i(1)) - \\hat{E}(Y_i(0))\\)"
  },
  {
    "objectID": "regression/regression.html#linear-regression",
    "href": "regression/regression.html#linear-regression",
    "title": "Regression and Standardization",
    "section": "Linear regression",
    "text": "Linear regression\nExample, using simulated data:\n\nn <- 800\nC <- rnorm(n)\nX <- rbinom(n, 1, plogis(-1 + 2 * C))\nY <- 1 + 1 * X + C + .5 * X * C + rnorm(n)\nsimdata <- data.frame(C, Y, X)\n\nfit <- glm(Y ~ C + X + C:X, data = simdata)\nsimdata0 <- simdata1 <- simdata\n\nsimdata0$X <- 0\nsimdata1$X <- 1\n\nEYi_1 <- predict(fit, newdata = simdata1)\nEYi_0 <- predict(fit, newdata = simdata0)\n\nmean(EYi_1) - mean(EYi_0)\n\n[1] 1.048538"
  },
  {
    "objectID": "regression/regression.html#inference",
    "href": "regression/regression.html#inference",
    "title": "Regression and Standardization",
    "section": "Inference?",
    "text": "Inference?\nOne option is to use the bootstrap:\n\nB <- 1000\nbootests <- rep(NA, B)\nfor(i in 1:B) {\n  simdata.star <- simdata[sample(1:nrow(simdata), \n                                 nrow(simdata), replace = TRUE), ]\n\n  fit <- glm(Y ~ C + X + C:X, data = simdata.star)\n  simdata0 <- simdata1 <- simdata.star\n\n  simdata0$X <- 0\n  simdata1$X <- 1\n\n  bootests[i] <- mean(predict(fit, newdata = simdata1)) - \n    mean(predict(fit, newdata = simdata0))\n}\n\nsd(bootests)\n\n[1] 0.1003283\n\nquantile(bootests, c(0.025, 0.9756))\n\n     2.5%    97.56% \n0.8594518 1.2515164 \n\n\nOr you can use the stdReg package\n\nlibrary(\"stdReg\")\n\nstdfit <- stdGlm(fit, data = simdata, X = \"X\", x = c(0, 1))\nsummary(stdfit, contrast = \"difference\", reference = 0)\n\n\nFormula: Y ~ C + X + C:X\nFamily: gaussian \nLink function: identity \nExposure:  X \nReference level:  X = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0    0.000      0.000      0.000      0.000\n1    0.734      0.106      0.526      0.943"
  },
  {
    "objectID": "regression/regression.html#logistic-regression",
    "href": "regression/regression.html#logistic-regression",
    "title": "Regression and Standardization",
    "section": "Logistic regression",
    "text": "Logistic regression\nWith a binary outcome, we would tend to use logistic regression.\n\\[\n\\mbox{logit}(p\\{Y = 1 | X, C\\}) = \\beta_0 + \\beta_1 X + \\beta_2 C\n\\]\nbut the coefficient \\(\\beta_1\\) is not a marginal causal contrast, i.e., there is no function \\(g\\) such that\n\\[\n\\beta_1 = g\\{Y(1), Y(0)\\},\n\\] the reason being that the nonlinear function does not simplify, so that it continues to depend on \\(C\\), it is a conditional causal contrast. Besides, we want the risk difference or risk ratio, so we need to standardize with logistic regression."
  },
  {
    "objectID": "regression/regression.html#binary-example",
    "href": "regression/regression.html#binary-example",
    "title": "Regression and Standardization",
    "section": "Binary example",
    "text": "Binary example\nThe steps are nearly identical:\n\nn <- 800\nC <- rnorm(n)\nX <- rbinom(n, 1, plogis(-1 + 2 * C))\nY <- rbinom(n, 1, plogis(1 + 1 * X + C + .5 * X * C))\nsimdata <- data.frame(C, Y, X)\n\nfit <- glm(Y ~ C + X + C:X, data = simdata, family = \"binomial\")\nsimdata0 <- simdata1 <- simdata\n\nsimdata0$X <- 0\nsimdata1$X <- 1\n\nEYi_1 <- predict(fit, newdata = simdata1, type = \"response\")\nEYi_0 <- predict(fit, newdata = simdata0, type = \"response\")\n\nmean(EYi_1) - mean(EYi_0)\n\n[1] 0.1169359\n\nmean(EYi_1) / mean(EYi_0)\n\n[1] 1.16908\n\n\nOr use the stdReg package\n\nstdfit <- stdGlm(fit, X = \"X\", x = c(0,1), data = simdata)\nsummary(stdfit, contrast = \"difference\", reference = 0)\n\n\nFormula: Y ~ C + X + C:X\nFamily: binomial \nLink function: logit \nExposure:  X \nReference level:  X = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0    0.000       0.00     0.0000      0.000\n1    0.117       0.05     0.0189      0.215\n\nsummary(stdfit, contrast = \"ratio\", reference = 0)\n\n\nFormula: Y ~ C + X + C:X\nFamily: binomial \nLink function: logit \nExposure:  X \nReference level:  X = 0 \nContrast:  ratio \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     1.00     0.0000       1.00       1.00\n1     1.17     0.0741       1.02       1.31"
  },
  {
    "objectID": "regression/regression.html#cox-regression-and-other-survival-models",
    "href": "regression/regression.html#cox-regression-and-other-survival-models",
    "title": "Regression and Standardization",
    "section": "Cox regression and other survival models",
    "text": "Cox regression and other survival models\nFor a time to event outcome \\(Y\\), the Cox model assumes \\[\n\\lim_{\\delta \\rightarrow 0} \\frac{1}{\\delta}p\\{t \\leq Y < t + \\delta | Y \\geq t, X, C\\} = \\lambda_0(t) \\exp(\\beta_1 X + \\beta_2 C)\n\\]\n\\(\\beta_1\\) is not generally a causal contrast, for different reasons than the logistic model. Hence we need to standardize to get a contrast of survival probabilities such as \\[\np\\{Y(1) > \\tau\\} - p\\{Y(0) > \\tau\\}\n\\] for a given time \\(\\tau\\).\nThe standardization procedure is basically the same\n\n\n\n\n\n\nNote\n\n\nTo be pedantic, to get a prediction of the survival probability, you also need the Breslow estimator of the baseline hazard. Note that the predictions are still subject to the proportional hazards assumption."
  },
  {
    "objectID": "regression/regression.html#cox-example",
    "href": "regression/regression.html#cox-example",
    "title": "Regression and Standardization",
    "section": "Cox example",
    "text": "Cox example\n\nlibrary(survival)\nhead(rotterdam)\n\n     pid year age meno  size grade nodes pgr  er hormon chemo rtime recur dtime\n1393   1 1992  74    1  <=20     3     0  35 291      0     0  1799     0  1799\n1416   2 1984  79    1 20-50     3     0  36 611      0     0  2828     0  2828\n2962   3 1983  44    0  <=20     2     0 138   0      0     0  6012     0  6012\n1455   4 1985  70    1 20-50     3     0   0  12      0     0  2624     0  2624\n977    5 1983  75    1  <=20     3     0 260 409      0     0  4915     0  4915\n617    6 1983  52    0  <=20     3     0 139 303      0     0  5888     0  5888\n     death\n1393     0\n1416     0\n2962     0\n1455     0\n977      0\n617      0\n\nsdata <- rotterdam\n\ncfit <- coxph(Surv(dtime, death) ~ hormon + age + meno + grade + er, \n              data = sdata, ties = \"breslow\")\nsdata0 <- sdata1 <- sdata\nsdata0$hormon <- 0\nsdata1$hormon <- 1\n\nEy0t <- survfit(cfit, newdata = sdata0, se.fit = FALSE)\nEy1t <- survfit(cfit, newdata = sdata1, se.fit = FALSE)\n\nmean(Ey1t$surv[max(which(Ey1t$time <= 3000)), ]) - \n  mean(Ey0t$surv[max(which(Ey0t$time <= 3000)), ])\n\n[1] -0.07465547\n\n\nor use stdCoxph\n\nstdcfit <- stdCoxph(cfit, data = sdata, X = \"hormon\", x = c(0, 1), \n                    t = 3000)\nsummary(stdcfit, contrast = \"difference\", reference = 0)\n\n\nFormula: Surv(dtime, death) ~ hormon + age + meno + grade + er\nExposure:  hormon \nReference level:  hormon = 0 \nContrast:  difference \nSurvival functions evaluated at t = 3000 \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0   0.0000     0.0000      0.000      0.000\n1  -0.0747     0.0274     -0.128     -0.021"
  },
  {
    "objectID": "regression/regression.html#summary-and-comments",
    "href": "regression/regression.html#summary-and-comments",
    "title": "Regression and Standardization",
    "section": "Summary and comments",
    "text": "Summary and comments\n\nRegression standardization is a method to estimate causal effects, for all types of exposures (binary, categorical, continuous)\nIt works when the outcome model is correctly specified for confounding, i.e., all confounders are included in the model in the correct functional form (nonlinearities, interactions, etc.)\nBasic steps:\n\nFit an outcome regression model (can also fit two, separately by the exposure levels). Use diagnostics to make sure the model is as flexible as it needs to be\nCreate copies of the data, and set the exposure levels to the desired levels for the contrast\nPredict the potential outcome for each individual given their observed covariates\nTake the means and construct the contrast of interest (difference or ratio)"
  },
  {
    "objectID": "regression/regression.html#next-time",
    "href": "regression/regression.html#next-time",
    "title": "Regression and Standardization",
    "section": "Next time",
    "text": "Next time"
  },
  {
    "objectID": "hazards/hazards-pres.html#defining-features-of-survival-data",
    "href": "hazards/hazards-pres.html#defining-features-of-survival-data",
    "title": "Hazards of hazard ratios",
    "section": "Defining features of survival data",
    "text": "Defining features of survival data\n\nA continuous time variable\n\nResearch interest is on the length of time elapsing between two events.\nTime from diagnosis to event = length of follow up\n\nA binary event variable\n\nIndicates whether the event of interest occurred (1 = yes, 0 = no)\nIf 1, the time is the time at which the event occurred\nIf 0, the last follow-up time = censored\nCensored means that the event occurs after the recorded follow-up time\n\nPossibly useful information!"
  },
  {
    "objectID": "hazards/hazards-pres.html#the-survival-function",
    "href": "hazards/hazards-pres.html#the-survival-function",
    "title": "Hazards of hazard ratios",
    "section": "The survival function",
    "text": "The survival function\n\nDefinition: \\(S(t) = P(T > t) =\\) probability that the event of interest occurs after time \\(t\\)\nProperties\n\n\\(0 \\leq S(t) \\leq 1\\) because \\(S(t)\\) is a probability.\n\\(S(0) = 1\\) because everyone is event-free at baseline.\n\\(S(b) \\leq S(a)\\) if \\(a \\leq b\\). The survival probability does not increase with time."
  },
  {
    "objectID": "hazards/hazards-pres.html#information-in-the-survival-function",
    "href": "hazards/hazards-pres.html#information-in-the-survival-function",
    "title": "Hazards of hazard ratios",
    "section": "Information in the survival function",
    "text": "Information in the survival function\n\nThe \\(S(t)\\) curve contains all information on how the survival probability changes over time\nWe can use it to compute:\n\nThe 1-year survival probability\nSurvival quantiles (e.g., median)\nThe mean survival"
  },
  {
    "objectID": "hazards/hazards-pres.html#example-lung-cancer-data",
    "href": "hazards/hazards-pres.html#example-lung-cancer-data",
    "title": "Hazards of hazard ratios",
    "section": "Example (lung cancer data)",
    "text": "Example (lung cancer data)"
  },
  {
    "objectID": "hazards/hazards-pres.html#the-hazard-function",
    "href": "hazards/hazards-pres.html#the-hazard-function",
    "title": "Hazards of hazard ratios",
    "section": "The hazard function",
    "text": "The hazard function\nDefinition:\n\n\\(h(t)\\) = instantaneous rate of death at time \\(t\\)\nThe probability of death per time-unit at \\(t\\), among those who survived until just before \\(t\\)\n\\(h(t) = \\lim_{\\delta \\rightarrow 0} \\frac{P(t \\leq T < t + \\delta | T \\geq t)}{\\delta}.\\)\n\nProperties:\n\n\\(h(t) = -(d/dt S(t))/(S(t)) = f(t) / S(t)\\) where \\(f(t)\\) is the probability density function\n\\(S(t) = \\exp(-\\int_0^t h(x) \\, dx)\\)\nRange: \\([0, \\infty)\\)\nIs related to survival function, but different scale\nNot a probability, a rate"
  },
  {
    "objectID": "hazards/hazards-pres.html#basic-cox-proportional-hazards-model-cox-d.r.-1972-jrss-b",
    "href": "hazards/hazards-pres.html#basic-cox-proportional-hazards-model-cox-d.r.-1972-jrss-b",
    "title": "Hazards of hazard ratios",
    "section": "Basic Cox proportional hazards model (Cox, D.R., 1972, JRSS-B)",
    "text": "Basic Cox proportional hazards model (Cox, D.R., 1972, JRSS-B)\nFor a covariate \\(x\\), the basic Cox model assumes\n\\[h(t | x) = h_0(t) \\exp(\\beta_1 \\cdot x)\\]\nwhere \\(h_0(t)\\) is the baseline hazard function.\nAssumption: Proportional hazards (PH).\n\nThe hazard function for any covariate value \\(x\\) is proportional to the baseline hazard \\(h_0(t)\\)\nThe multiplicative factor is called the hazard ratio, which does not depend on time \\(t\\)\nThe coefficient \\(\\beta_1\\) is called the log hazard ratio.\n\n\\[\n\\frac{h(t | x = 1)}{h(t | x = 0)} = \\frac{h_0(t) \\exp(\\beta_1 \\cdot 1)}{h_0(t) \\exp(\\beta_1 \\cdot 0)} = \\exp(\\beta_1)\n\\]"
  },
  {
    "objectID": "hazards/hazards-pres.html#the-baseline-hazard",
    "href": "hazards/hazards-pres.html#the-baseline-hazard",
    "title": "Hazards of hazard ratios",
    "section": "The baseline hazard",
    "text": "The baseline hazard\n\\[h(t | x = 0) = h_0(t) \\exp(\\beta_1 \\cdot 0) = h_0(t).\\]\n\n\\(h_0(t)\\) is the hazard function for the stratum with exposure = 0.\n\nThis does depend on time\nWe make no assumptions on what the shape of the function is: it is a semi-parametric model"
  },
  {
    "objectID": "hazards/hazards-pres.html#in-his-own-words",
    "href": "hazards/hazards-pres.html#in-his-own-words",
    "title": "Hazards of hazard ratios",
    "section": "In his own words",
    "text": "In his own words\n\n“The present paper is largely concerned with the extension of the results of Kaplan and Meier to the … incorporation of regression-like arguments into life-table analysis.”\n\n\n“A simple form for the hazard is not by itself particularly advantageous, and models other than [Cox’s] may be more natural.”\n\n\n“In the present paper we shall, however, concentrate on exploring the consequence of allowing \\(h_0(t)\\) to be arbitrary, main interest being in the regression parameters”\n\nCox, D. R. Regression models and life tables, J. R. Stat. Soc. B 34, 187–220 (1972).\nAbout 50,000 citations."
  },
  {
    "objectID": "hazards/hazards-pres.html#why-is-the-cox-model-so-popular",
    "href": "hazards/hazards-pres.html#why-is-the-cox-model-so-popular",
    "title": "Hazards of hazard ratios",
    "section": "Why is the Cox model so popular?",
    "text": "Why is the Cox model so popular?\n\nEasy to use, available in all stats software packages\nComputationally nice, rarely fails or gives warnings\nSimple – it reduces something complex to a single number\nTheoretically interesting for statisticians – the Cox partial likelihood led to new theoretical development in the field (e.g. PK Andersen’s work)"
  },
  {
    "objectID": "hazards/hazards-pres.html#causal-inference",
    "href": "hazards/hazards-pres.html#causal-inference",
    "title": "Hazards of hazard ratios",
    "section": "Causal inference",
    "text": "Causal inference\n\nCausal effects can be thought of as the change in the summary statistic of an outcome variable associated with the manipulation of an exposure variable from/to a particular level.\nWe have observations, which are usually generated via mechanisms that do not involve manipulation of the exposure.\nThe goal is to use the distribution of the observations to inform us about the causal effect of interest"
  },
  {
    "objectID": "hazards/hazards-pres.html#dags-and-observations",
    "href": "hazards/hazards-pres.html#dags-and-observations",
    "title": "Hazards of hazard ratios",
    "section": "DAGs and observations",
    "text": "DAGs and observations\nA directed acyclic graph (or DAG or just graph) conveys our assumptions about the mechanisms that gave rise to the observations, e.g.,\n\nThis is a functional causal model \\(\\{F_V:pa(V)\\times U_V\\to V\\mid V\\in\\mathcal{V}\\}\\), e.g., \\(y = F_Y(x, u, \\varepsilon_Y)\\)."
  },
  {
    "objectID": "hazards/hazards-pres.html#effects",
    "href": "hazards/hazards-pres.html#effects",
    "title": "Hazards of hazard ratios",
    "section": "Effects",
    "text": "Effects\nA causal effects will be written as e.g., \\(p\\{Y(X = 1) = 1\\} - p\\{Y(X = 0) = 1\\}\\), where \\(Y(X = 1)\\) is the potential outcome which means\n“the variable \\(Y\\) if \\(X\\) were intervened upon to have value 1”."
  },
  {
    "objectID": "hazards/hazards-pres.html#back-to-hazard-ratios",
    "href": "hazards/hazards-pres.html#back-to-hazard-ratios",
    "title": "Hazards of hazard ratios",
    "section": "Back to hazard ratios",
    "text": "Back to hazard ratios\n\nWe often are interested the causal effect on a time-to-event outcome with right censoring in our study (examples?)\nThe flow-chart approach to statistics tells us that we should use the Cox model in this situation, which gives us the hazard ratio (maybe adjusted for confounders), which we interpret as our estimated causal effect.\n\nWhat are the two problems that Hernán points out in this situation?"
  },
  {
    "objectID": "hazards/hazards-pres.html#the-problems",
    "href": "hazards/hazards-pres.html#the-problems",
    "title": "Hazards of hazard ratios",
    "section": "The problems",
    "text": "The problems\n\nThe real hazard ratios probably depend on time\nBuilt-in selection bias\n\nBoth problems lead to difficulties in interpreting the HR as a causal effect (unless it equals 1)"
  },
  {
    "objectID": "hazards/hazards-pres.html#illustration-by-torben-martinussen",
    "href": "hazards/hazards-pres.html#illustration-by-torben-martinussen",
    "title": "Hazards of hazard ratios",
    "section": "Illustration (by Torben Martinussen)",
    "text": "Illustration (by Torben Martinussen)\nSuppose \\[\n\\lambda(t;X)=\\{e^{\\beta_1X}I(t\\leq 4)+ e^{\\beta_2X}I(t>4)\\} \\lambda_0(t)\n\\] is the true hazard. One HR on \\([0,4]\\) (beneficial on this range) and another one on \\((4,\\tau [\\) (no effect on this range).\nLet’s also assume that we are in a randomized trial, so no variables impact the treatment assignment.\nIs it possible that other variables may impact survival time? Yes!"
  },
  {
    "objectID": "hazards/hazards-pres.html#frailty",
    "href": "hazards/hazards-pres.html#frailty",
    "title": "Hazards of hazard ratios",
    "section": "Frailty",
    "text": "Frailty\nFrailty is the term for a random effect that affects survival, in our model we may have\n\\[\n\\lambda(t;X,Z)=Z\\lambda^*(t;X)\n\\]\nwhere \\(Z\\) is a random variable with mean and variance 1. In this case we can derive the HR as a function of \\(Z\\), which is closer to a causal effect (see DAG in next slide)"
  },
  {
    "objectID": "hazards/hazards-pres.html#effect-given-z",
    "href": "hazards/hazards-pres.html#effect-given-z",
    "title": "Hazards of hazard ratios",
    "section": "Effect given \\(Z\\)",
    "text": "Effect given \\(Z\\)"
  },
  {
    "objectID": "hazards/hazards-pres.html#dag-in-this-setting",
    "href": "hazards/hazards-pres.html#dag-in-this-setting",
    "title": "Hazards of hazard ratios",
    "section": "DAG in this setting",
    "text": "DAG in this setting"
  },
  {
    "objectID": "hazards/hazards-pres.html#summary",
    "href": "hazards/hazards-pres.html#summary",
    "title": "Hazards of hazard ratios",
    "section": "Summary",
    "text": "Summary\nKeep in mind that we do not obtain biased estimates, but arguing about the treatment effect using the hazard ratio simply leads to a wrong conclusion\n\nExchangeability due to randomization only holds at time \\(t=0\\), it is lost when \\(t>0\\).\nWe have, for \\(t\\le v\\), that \\[\nE(Z|T>t,X=x)= \\left\\{ \\begin{array}{ll}\n\\exp{\\{-\\Lambda_0(t)\\}} & \\textrm{if $x=0$}\\\\\n\\exp{\\{-\\Lambda_0(t)e^{\\beta_1}\\}}  & \\textrm{if $x=1$}\\\\\n\\end{array} \\right.\n\\] so indeed selection is taking place: \\[\nE(Z|T>t,X=1)>E(Z|T>t,X=0).\n\\]"
  },
  {
    "objectID": "hazards/hazards-pres.html#frailty-selection",
    "href": "hazards/hazards-pres.html#frailty-selection",
    "title": "Hazards of hazard ratios",
    "section": "Frailty selection",
    "text": "Frailty selection"
  },
  {
    "objectID": "hazards/hazards-pres.html#solutions",
    "href": "hazards/hazards-pres.html#solutions",
    "title": "Hazards of hazard ratios",
    "section": "Solutions",
    "text": "Solutions\nWhat do? What does Hernán suggest?\nUse “cumulative” estimands, that have nice causal interpretations, e.g.,\nDifferences or ratios in survival probabilities at fixed times: \\(P(T > t | X = 1) - P(T > t | X = 0)\\)\nDifferences or ratios in restricted mean survival."
  },
  {
    "objectID": "hazards/hazards-pres.html#practicalities",
    "href": "hazards/hazards-pres.html#practicalities",
    "title": "Hazards of hazard ratios",
    "section": "Practicalities",
    "text": "Practicalities\nIPTW survival curves to adjust for confounding (how to do inference?)\nMore options:\n\nstdReg R package, to do standardization to adjust for covariate while estimating survival probabilities\neventglm R package, regression modeling of survival probabilities and restricted mean survival\ntimereg R package, direct modeling of the same\nAccelerated failure time models, which directly model effects on survival (see reference 5)"
  }
]